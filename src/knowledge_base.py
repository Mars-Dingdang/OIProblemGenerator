import os
import glob
import torch
from typing import List
from tqdm import tqdm
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document

class KnowledgeBase:
    def __init__(self, persist_directory: str = "./chroma_db"):
        self.persist_directory = persist_directory
        # ä½¿ç”¨æœ¬åœ° HuggingFace æ¨¡å‹ç”Ÿæˆ Embeddingï¼Œé¿å…æ¶ˆè€— API é¢åº¦ä¸”é€Ÿåº¦å¿«
        # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡ (GPU/CPU)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ–¥ï¸ Using device for embeddings: {device}")

        self.embedding_function = HuggingFaceEmbeddings(
            model_name="all-MiniLM-L6-v2",
            model_kwargs={'device': device}
        )
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        if os.path.exists(persist_directory):
            self.vector_store = Chroma(
                persist_directory=persist_directory,
                embedding_function=self.embedding_function
            )
        else:
            self.vector_store = None

    def retrieve(self, query: str, k: int = 3) -> str:
        """
        æ ¹æ®æŸ¥è¯¢æ£€ç´¢ç›¸å…³çŸ¥è¯†
        """
        context_parts = []
        
        # 1. å°è¯•ç›´æ¥åŒ¹é…ç»“æ„åŒ–çŸ¥è¯†åº“ä¸­çš„ç®—æ³•
        structured_info = self.get_algorithm_info(query)
        if structured_info:
            print(f"ğŸ¯ Found structured knowledge for: {query}")
            context_parts.append(f"=== Structured Knowledge for {query} ===\n{structured_info}")
        
        # 2. å‘é‡æ£€ç´¢
        if self.vector_store:
            print(f"ğŸ” Searching vector database for: {query}")
            results = self.vector_store.similarity_search(query, k=k)
            vector_context = "\n\n".join([f"--- Source: {doc.metadata.get('source', 'Unknown')} ---\n{doc.page_content}" for doc in results])
            context_parts.append(f"=== Retrieved Context ===\n{vector_context}")
        else:
            context_parts.append("Vector database not initialized.")
            
        return "\n\n".join(context_parts)

    def get_algorithm_info(self, topic: str) -> str:
        """
        ç›´æ¥ä»ç»“æ„åŒ–ç›®å½•ä¸­æŸ¥æ‰¾ç®—æ³•ä¿¡æ¯
        """
        import json
        structured_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "knowledge_base", "structured")
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        topic_lower = topic.lower().replace(" ", "_")
        
        found_content = []
        
        for root, dirs, files in os.walk(structured_dir):
            if "metadata.json" in files:
                # æ£€æŸ¥æ–‡ä»¶å¤¹åæ˜¯å¦åŒ¹é…
                folder_name = os.path.basename(root).lower()
                
                # æ£€æŸ¥ metadata ä¸­çš„ name æ˜¯å¦åŒ¹é…
                try:
                    with open(os.path.join(root, "metadata.json"), "r", encoding="utf-8") as f:
                        meta = json.load(f)
                    algo_name = meta.get("name", "").lower()
                except:
                    algo_name = ""
                
                # åŒ¹é…é€»è¾‘ï¼šæ–‡ä»¶å¤¹ååŒ…å« topic æˆ– topic åŒ…å«æ–‡ä»¶å¤¹åï¼Œæˆ–è€… name åŒ¹é…
                if topic_lower in folder_name or folder_name in topic_lower or topic_lower in algo_name or algo_name in topic_lower:
                    # æ‰¾åˆ°äº†ï¼æå–æ‰€æœ‰ä¿¡æ¯
                    info = f"Algorithm: {meta.get('name')}\n"
                    info += f"Category: {meta.get('category')}\n"
                    info += f"Complexity: {meta.get('complexity')}\n"
                    
                    if "tricks.md" in files:
                        with open(os.path.join(root, "tricks.md"), "r", encoding="utf-8") as f:
                            info += f"\n--- Tricks & Observations ---\n{f.read()}\n"
                            
                    if "template.cpp" in files:
                        with open(os.path.join(root, "template.cpp"), "r", encoding="utf-8") as f:
                            info += f"\n--- Standard Template ---\n```cpp\n{f.read()}\n```\n"
                    
                    found_content.append(info)
        
        return "\n\n".join(found_content)

    def ingest_documents(self, source_dir: str):
        """
        ä»æŒ‡å®šç›®å½•è¯»å–æ–‡æ¡£å¹¶å­˜å…¥å‘é‡æ•°æ®åº“
        """
        print(f"ğŸ“‚ Scanning {source_dir} for documents...")
        
        documents = []
        
        # é€’å½’æŸ¥æ‰¾æ‰€æœ‰ PDF æ–‡ä»¶
        pdf_files = []
        for root, dirs, files in os.walk(source_dir):
            for file in files:
                if file.lower().endswith(".pdf"):
                    pdf_files.append(os.path.join(root, file))
        
        print(f"ğŸ“„ Found {len(pdf_files)} PDF files.")
        
        for pdf_path in tqdm(pdf_files, desc="Loading PDFs"):
            try:
                loader = PyMuPDFLoader(pdf_path)
                documents.extend(loader.load())
            except Exception as e:
                print(f"âš ï¸ Error loading {pdf_path}: {e}")

        # å¤„ç†ç»“æ„åŒ–æ•°æ® (Structured Data)
        structured_dir = os.path.join(source_dir, "structured")
        if os.path.exists(structured_dir):
            print(f"ğŸ“‚ Scanning structured data in {structured_dir}...")
            structured_docs = self._load_structured_data(structured_dir)
            documents.extend(structured_docs)
            print(f"ğŸ“„ Added {len(structured_docs)} structured documents.")

        if not documents:
            print("âŒ No documents found to ingest.")
            return

        print(f"âœ‚ï¸ Splitting {len(documents)} pages into chunks...")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", " ", ""]
        )
        chunks = text_splitter.split_documents(documents)
        
        print(f"ğŸ’¾ Storing {len(chunks)} chunks into ChromaDB...")
        
        # Batch processing to avoid memory issues and show progress
        batch_size = 100
        for i in tqdm(range(0, len(chunks), batch_size), desc="Embedding Batches"):
            batch = chunks[i:i + batch_size]
            if self.vector_store is None:
                self.vector_store = Chroma.from_documents(
                    documents=batch,
                    embedding=self.embedding_function,
                    persist_directory=self.persist_directory
                )
            else:
                self.vector_store.add_documents(batch)
                
        print("âœ… Knowledge base built successfully!")

    def _load_structured_data(self, root_dir: str) -> List[Document]:
        """
        åŠ è½½ç»“æ„åŒ–çŸ¥è¯†åº“ (Level 1/2/3)
        """
        docs = []
        import json
        
        for root, dirs, files in os.walk(root_dir):
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ metadata.jsonï¼Œå¦‚æœå­˜åœ¨åˆ™è®¤ä¸ºæ˜¯ä¸€ä¸ªç®—æ³•æ–‡ä»¶å¤¹
            if "metadata.json" in files:
                try:
                    with open(os.path.join(root, "metadata.json"), "r", encoding="utf-8") as f:
                        meta = json.load(f)
                    
                    algo_name = meta.get("name", "Unknown Algorithm")
                    
                    # Level 1: Metadata
                    content_l1 = f"Algorithm: {algo_name}\nCategory: {meta.get('category')}\n"
                    content_l1 += f"Complexity: {meta.get('complexity')}\n"
                    content_l1 += f"Scenarios: {', '.join(meta.get('scenarios', []))}\n"
                    content_l1 += f"Extensions: {', '.join(meta.get('extensions', []))}"
                    
                    docs.append(Document(
                        page_content=content_l1,
                        metadata={"source": f"Structured KB - {algo_name} - Metadata", "type": "level1", "algorithm": algo_name}
                    ))
                    
                    # Level 2: Template
                    if "template.cpp" in files:
                        with open(os.path.join(root, "template.cpp"), "r", encoding="utf-8") as f:
                            template_code = f.read()
                        docs.append(Document(
                            page_content=f"Standard Template Code for {algo_name}:\n```cpp\n{template_code}\n```",
                            metadata={"source": f"Structured KB - {algo_name} - Template", "type": "level2", "algorithm": algo_name}
                        ))
                        
                    # Level 3: Tricks
                    if "tricks.md" in files:
                        with open(os.path.join(root, "tricks.md"), "r", encoding="utf-8") as f:
                            tricks_content = f.read()
                        docs.append(Document(
                            page_content=f"Advanced Tricks and Key Observations for {algo_name}:\n{tricks_content}",
                            metadata={"source": f"Structured KB - {algo_name} - Tricks", "type": "level3", "algorithm": algo_name}
                        ))
                        
                except Exception as e:
                    print(f"âš ï¸ Error loading structured data in {root}: {e}")
                    
        return docs
